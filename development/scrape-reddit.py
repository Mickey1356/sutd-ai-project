import datetime as dt
import requests
import json
import os
import glob
import csv
import pandas as pd
import praw
from tqdm import tqdm
from decouple import config

def scrape_wsb(date):
    """Scrapes Pushshift (https://github.com/pushshift/api) for r/wallstreetbets posts for a given date.
    Saves the author (`a`), created timestamp (`c`), submission id (`i`), title (`t`), post contents (`p`), and stored number of comments (`n`) in a json file.
    The number of comments may be inaccurate.
    The file will be saved as `data/raw/[date].json`.

    Args:
        date (string): The date (DD-MM-YYYY) for which to get the posts to the r/wallstreetbets subreddit.

    Returns:
        int: The total number of posts posted to r/wallstreetbets on the given date.
    """    
    if not os.path.exists('data/raw/'):
        os.makedirs('data/raw/')

    start_date = f'{date} 00:00:00 +0000'
    end_date = f'{date} 23:59:59 +0000'

    utc_from = int(dt.datetime.strptime(start_date, "%d-%m-%Y %H:%M:%S %z").timestamp())
    utc_to = int(dt.datetime.strptime(end_date, "%d-%m-%Y %H:%M:%S %z").timestamp())

    # print(utc_from, utc_to)

    req_url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=wallstreetbets&after={}&before={}&size=500'

    results = []
    tl_submissions = 0

    while utc_from < utc_to:
        request = req_url.format(utc_from, utc_to)

        r = requests.get(request)
        submissions = r.json()['data']
        if len(submissions) > 0:
            tl_submissions += len(submissions)
            for submission in submissions:
                author = submission['author']
                created = dt.datetime.utcfromtimestamp(submission['created_utc']).strftime('%d-%m-%Y_%H:%M:%S')
                sub_id = submission['id']
                title = submission['title']
                post = submission.get('selftext', '')
                n_comments = submission.get('num_comments', 0)

                trimmed = {'a': author, 'c': created, 'i': sub_id, 't': title, 'p': post, 'n': n_comments}
                results.append(trimmed)
            last_created = submissions[-1]['created_utc']
            utc_from = last_created
            # print(utc_from)
        else:
            break

    with open(f'data/raw/{date}.json', 'w') as f:
        json.dump(results, f)
    
    return tl_submissions

# combines the results from pushshift into a csv files representing each month
def combine(month):
    """Combines the files generated from `scrape_wsb` into a single file for a given month.
    Filters out empty posts (deleted, removed or image-only posts).
    Saves the results as a CSV file, found in `data/[month].csv`.

    Args:
        month (string): The month (MM) for which to combine the files
    """    
    files = glob.glob(f'data/raw/*-{month}-2021.json')

    empty_post = ['', '[removed]', '[deleted]']
    filtered = []

    for fname in files:
        with open(fname, encoding='utf-8') as f:
            daily_subs = json.load(f)

            for sub in daily_subs:
                if not sub.get('p', '') in empty_post:
                    sub['p'] = ' '.join(sub['p'].split())
                    sub['n'] = sub.get('n', 0)
                    filtered.append(sub)

    print(len(filtered))
    with open(f'data/{month}.csv', 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, filtered[0].keys())
        writer.writeheader()
        writer.writerows(filtered)

def update_reddit(month):
    """Opens the CSV file of a specified month generated by `combine`, and scrapes Reddit (using the official API) to get certain updated information.
    Also filters out posts that do not contain any mention of the BANG stocks.
    The updated information obtained from Reddit are the number of comments (`n`), the score (`s`), as well as the upvote ratio (`r`).
    Saves the combined results as a new CSV file, found in `data/[month]-updated.csv`.

    Args:
        month (string): The month (MM) for which to get update from Reddit
    """  
    # load a reddit instance with appropriate authentication keys
    reddit = praw.Reddit(
        client_id=config('CLIENT_ID'),
        client_secret=config('CLIENT_SECRET'),
        user_agent=config('CLIENT_NAME')
    )

    # first filter out non-bang stocks
    df = pd.read_csv(f'data/{month}.csv')

    df['t'] = df['t'].str.lower()
    df['p'] = df['p'].str.lower()

    bb = r'\bbb|blackberry\b'
    amc = r'\bamc\b'
    nok = r'\bnokia|nok\b'
    gme = r'\bgamestop|gme\b'

    df['bb']  = df['p'].str.contains(bb,  regex=True) | df['t'].str.contains(bb,  regex=True)
    df['amc'] = df['p'].str.contains(amc, regex=True) | df['t'].str.contains(amc, regex=True)
    df['nok'] = df['p'].str.contains(nok, regex=True) | df['t'].str.contains(nok, regex=True)
    df['gme'] = df['p'].str.contains(gme, regex=True) | df['t'].str.contains(gme, regex=True)

    df['any'] = df['bb'] | df['amc'] | df['nok'] | df['gme']

    df = df.drop(df[df['any'] == False].index)
    df = df.reset_index(drop=True)

    # scrape reddit for updated scores and number of comments
    subs = [f't3_{df.at[i, "i"]}' for i in tqdm(df.index)]

    ss = {}
    cs = {}
    rs = {}

    for submission in tqdm(reddit.info(subs), total=len(subs)):
        m = submission.id
        s = submission.score
        c = submission.num_comments
        r = submission.upvote_ratio
        ss[m] = s
        cs[m] = c
        rs[m] = r

    df['s'] = df['i'].map(ss)
    df['n'] = df['i'].map(cs)
    df['r'] = df['i'].map(rs)

    # save each month to their own csv file
    df.to_csv(f'data/filtered/{m:02d}-updated.csv')
    print('Done')

if __name__ == '__main__':
    # specify the number of days for each month
    days = [31, 28, 31, 30, 31, 30]

    # iterate over from jan (1) to june (6)
    for m in range(1, 7):
        for day in range(1, days[m-1] + 1):
            date = f'{day:02d}-{m:02d}-2021'
            num_subs = scrape_wsb(date)
            print(f'{date} - {num_subs}')
    
        combine(f'{m:02d}')

        update_reddit(f'{m:02d}')