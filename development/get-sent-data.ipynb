{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Copy of 1-GetSenData.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Get Sentiment Data\n","\n","We tried out vaderSentiment (https://github.com/cjhutto/vaderSentiment) as one of the approaches used to generate sentiment data for use as features for our model.\n","\n","In this notebook, we used vaderSentiment to generate sentiment labels for every single post, and store them in a column under `sentiment`.\n","\n","For each sentiment label generated by vaderSentiment (-2, -1, 0, 1, 2), we sampled 100 posts in order to manually verify that the results are acceptable.\n","\n","Sentiment, by its very nature, is extremely subjective. From a cursory glance, sadly, we saw that the sampled posts (which would be generated by this notebook in `sentiment_df.csv`) usually did not match the sentiments assigned to them."],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["!pip install vaderSentiment"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 13.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byvYtE_mSuu0","executionInfo":{"status":"ok","timestamp":1628079690882,"user_tz":-480,"elapsed":4054,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}},"outputId":"2aa1785c-7254-4722-f220-b51d92c85a69"}},{"cell_type":"code","execution_count":2,"source":["import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","from collections import deque\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","analyser = SentimentIntensityAnalyzer()\n","\n","# Mount GDrive\n","drive.mount(\"/content/gdrive\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8moektB0quvN","executionInfo":{"status":"ok","timestamp":1628079721858,"user_tz":-480,"elapsed":30987,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}},"outputId":"1149ad0c-f5a9-4fbb-e172-46fd469778a6"}},{"cell_type":"code","execution_count":3,"source":["# Read data\n","df = pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/01-updated.csv\")\n","df = df.append(pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/02-updated.csv\"), ignore_index=True)\n","df = df.append(pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/03-updated.csv\"), ignore_index=True)\n","df = df.append(pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/04-updated.csv\"), ignore_index=True)\n","df = df.append(pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/05-updated.csv\"), ignore_index=True)\n","df = df.append(pd.read_csv(\"gdrive/MyDrive/AI Project/Reddit/06-updated.csv\"), ignore_index=True)"],"outputs":[],"metadata":{"id":"0UruNGr0P__b","executionInfo":{"status":"ok","timestamp":1628079730209,"user_tz":-480,"elapsed":8357,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}}}},{"cell_type":"code","execution_count":4,"source":["#score indication:  -2:strongly negative    -1:negative   0:neutral     1:positive    2:strongly positive\n","def sentiment_analyzer_scores(row):\n","    score = analyser.polarity_scores(row['p'])\n","    score = float(str(score['compound']))\n","    if score != 0:\n","        new_score = score * 2\n","        if new_score > 0:\n","          new_score += 0.5\n","        else:\n","          new_score -= 0.5\n","    else:\n","        new_score = 0\n","    return int(new_score)"],"outputs":[],"metadata":{"id":"o7sdktcUQADD","executionInfo":{"status":"ok","timestamp":1628079732453,"user_tz":-480,"elapsed":5,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}}}},{"cell_type":"code","execution_count":5,"source":["df['sentiment'] = df.apply(sentiment_analyzer_scores, axis=1)"],"outputs":[],"metadata":{"id":"9O_-gB-3Sej8","executionInfo":{"status":"ok","timestamp":1628080000362,"user_tz":-480,"elapsed":265787,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}}}},{"cell_type":"code","execution_count":6,"source":["max_sample_len = 100\n","\n","neg2 = deque(maxlen=max_sample_len)\n","neg1 = deque(maxlen=max_sample_len)\n","zero = deque(maxlen=max_sample_len)\n","pos1 = deque(maxlen=max_sample_len)\n","pos2 = deque(maxlen=max_sample_len)\n","\n","for index, row in df.iterrows():\n","    if row['sentiment'] == -2:\n","      neg2.append(row['p'])\n","    elif row['sentiment'] == -1:\n","      neg1.append(row['p'])\n","    elif row['sentiment'] == 0:\n","      zero.append(row['p'])\n","    elif row['sentiment'] == 1:\n","      pos1.append(row['p'])\n","    elif row['sentiment'] == 2:\n","      pos2.append(row['p'])"],"outputs":[],"metadata":{"id":"6cTRLgebScXs","executionInfo":{"status":"ok","timestamp":1628080006596,"user_tz":-480,"elapsed":6259,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}}}},{"cell_type":"code","execution_count":7,"source":["deque_list = [neg2, neg1, zero, pos1, pos2]\n","len_list = [len(neg2), len(neg1), len(zero), len(pos1), len(pos2)]\n","print(len_list)\n","merged_deque = deque()\n","for i in deque_list:\n","  merged_deque += i\n","\n","sentiment_df = pd.DataFrame(list(merged_deque), columns=[\"text\"])\n","\n","sentiment_list = []\n","\n","count = 0\n","for i in range(-2, 3):\n","  sentiment_list += [i] * len_list[count]\n","  count += 1\n","sentiment_df['sentiment'] = sentiment_list\n","\n","sentiment_df.to_csv(\"gdrive/MyDrive/AI Project/Reddit/sentiment_df.csv\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["[100, 100, 100, 100, 100]\n"]}],"metadata":{"id":"alpe5beOWD7O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628080007838,"user_tz":-480,"elapsed":1303,"user":{"displayName":"Heng jing han","photoUrl":"","userId":"01148339297681775778"}},"outputId":"a74f064e-f3a1-4e0c-fcfd-449738e65b85"}},{"cell_type":"markdown","source":["## Next is to verify the sentiment data, check if it make sense and then use it as the ground truth for training"],"metadata":{"id":"MRD5c9iuxvgv"}}]}